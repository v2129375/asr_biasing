{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/asr_biasing\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  6.12it/s]\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "Train dataset size: 1511\n",
      "Keywords directory: data/catslu\n",
      "training on 1 GPUs\n",
      "[2025-05-28 01:18:09,525] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "  0%|                                                   | 0/756 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/v2129375/asr_biasing/asr/finetune/finetune_speech_asr_keywords3.py\", line 436, in <module>\n",
      "    main() \n",
      "    ^^^^^^\n",
      "  File \"/home/v2129375/asr_biasing/asr/finetune/finetune_speech_asr_keywords3.py\", line 425, in main\n",
      "    trainer.train()\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2171, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3675, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3731, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py\", line 193, in forward\n",
      "    replicas = self.replicate(self.module, self.device_ids[: len(inputs)])\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py\", line 200, in replicate\n",
      "    return replicate(module, device_ids, not torch.is_grad_enabled())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/replicate.py\", line 126, in replicate\n",
      "    param_copies = _broadcast_coalesced_reshape(params, devices, detach)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/replicate.py\", line 95, in _broadcast_coalesced_reshape\n",
      "    tensor_copies = Broadcast.apply(devices, *tensors)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py\", line 23, in forward\n",
      "    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/comm.py\", line 66, in broadcast_coalesced\n",
      "    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "  0%|                                                   | 0/756 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python /home/v2129375/asr_biasing/asr/finetune/finetune_speech_asr_keywords3.py \\\n",
    "    --use_flash_attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:01<00:00,  2.33it/s]\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "Train dataset size: 1511\n",
      "Keywords directory: data/catslu\n",
      "training on 1 GPUs\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/v2129375/asr_biasing/asr/finetune/finetune_speech_asr_keywords3.py\", line 436, in <module>\n",
      "    main() \n",
      "    ^^^^^^\n",
      "  File \"/home/v2129375/asr_biasing/asr/finetune/finetune_speech_asr_keywords3.py\", line 418, in main\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 165, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 607, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 888, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3110, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1355, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 915, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 915, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 915, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 3 more times]\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 942, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1341, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.48 GiB of which 83.81 MiB is free. Including non-PyTorch memory, this process has 15.31 GiB memory in use. Of the allocated memory 14.79 GiB is allocated by PyTorch, and 277.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1 NCCL_DEBUG=INFO HF_DISABLE_TORCHDP=1 CUDA_VISIBLE_DEVICES=0 python \\\n",
    "    asr/finetune/finetune_speech_asr_keywords3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-28 14:51:41,505] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "W0528 14:51:42.341000 1376365 site-packages/torch/distributed/run.py:766] \n",
      "W0528 14:51:42.341000 1376365 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0528 14:51:42.341000 1376365 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0528 14:51:42.341000 1376365 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "[2025-05-28 14:51:44,168] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-28 14:51:44,169] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-28 14:51:44,869] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-28 14:51:44,871] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-28 14:51:44,871] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[rank1]:[W528 14:51:44.946720065 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  5.48it/s]\n",
      "[rank0]:[W528 14:51:54.882448331 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "ntumir-System-Product-Name:1376531:1376531 [0] NCCL INFO Bootstrap: Using eno1:140.112.174.204<0>\n",
      "ntumir-System-Product-Name:1376531:1376531 [0] NCCL INFO cudaDriverVersion 12080\n",
      "ntumir-System-Product-Name:1376531:1376531 [0] NCCL INFO NCCL version 2.26.5+cuda12.9\n",
      "ntumir-System-Product-Name:1376531:1376531 [0] NCCL INFO Comm config Blocking set to 1\n",
      "ntumir-System-Product-Name:1376532:1376532 [1] NCCL INFO cudaDriverVersion 12080\n",
      "ntumir-System-Product-Name:1376532:1376532 [1] NCCL INFO Bootstrap: Using eno1:140.112.174.204<0>\n",
      "ntumir-System-Product-Name:1376532:1376532 [1] NCCL INFO NCCL version 2.26.5+cuda12.9\n",
      "ntumir-System-Product-Name:1376532:1376532 [1] NCCL INFO Comm config Blocking set to 1\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO NET/IB : No device found.\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO NET/IB : Using [RO]; OOB eno1:140.112.174.204<0>\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO NET/Socket : Using [0]eno1:140.112.174.204<0> [1]wlp0s20f3:192.168.50.114<0>\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Using network Socket\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO ncclCommInitRankConfig comm 0x12f06e10 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0x83587af8357c892a - Init START\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO NET/IB : No device found.\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO NET/IB : Using [RO]; OOB eno1:140.112.174.204<0>\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO NET/Socket : Using [0]eno1:140.112.174.204<0> [1]wlp0s20f3:192.168.50.114<0>\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO Using network Socket\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO ncclCommInitRankConfig comm 0x23aa4df0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2000 commId 0x83587af8357c892a - Init START\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Bootstrap timings total 0.015094 (create 0.000014, send 0.000048, recv 0.002059, ring 0.000013, delay 0.000000)\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO Bootstrap timings total 0.013069 (create 0.000011, send 0.000034, recv 0.000066, ring 0.000425, delay 0.000000)\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO NCCL_SHM_DISABLE set by environment to 0.\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO NCCL_SHM_DISABLE set by environment to 0.\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO comm 0x23aa4df0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO comm 0x12f06e10 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Channel 00/02 : 0 1\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Channel 01/02 : 0 1\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0\n",
      "ntumir-System-Product-Name:1376532:1377073 [1] NCCL INFO [Proxy Service] Device 1 CPU core 25\n",
      "ntumir-System-Product-Name:1376531:1377074 [0] NCCL INFO [Proxy Service] Device 0 CPU core 16\n",
      "ntumir-System-Product-Name:1376531:1377076 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 24\n",
      "ntumir-System-Product-Name:1376532:1377075 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 23\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO ncclCommInitRankConfig comm 0x23aa4df0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2000 commId 0x83587af8357c892a - Init COMPLETE\n",
      "ntumir-System-Product-Name:1376532:1377066 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.14 (kernels 0.12, alloc 0.00, bootstrap 0.01, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO ncclCommInitRankConfig comm 0x12f06e10 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0x83587af8357c892a - Init COMPLETE\n",
      "ntumir-System-Product-Name:1376531:1377065 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.14 (kernels 0.12, alloc 0.00, bootstrap 0.02, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "ntumir-System-Product-Name:1376531:1377077 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376531:1377077 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376532:1377078 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376532:1377078 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376531:1377077 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "ntumir-System-Product-Name:1376532:1377078 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "Train dataset size: 755\n",
      "Keywords directory: data/catslu\n",
      "training on 2 GPUs\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using /home/v2129375/.cache/torch_extensions/py312_cu128 as PyTorch extensions root...\n",
      "Creating extension directory /home/v2129375/.cache/torch_extensions/py312_cu128/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/v2129375/.cache/torch_extensions/py312_cu128/cpu_adam/build.ninja...\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  5.22it/s]\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "training on 2 GPUs\n",
      "Using /home/v2129375/.cache/torch_extensions/py312_cu128 as PyTorch extensions root...\n",
      "[1/3] /home/v2129375/miniconda3/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/home/v2129375/miniconda3/lib/python3.12/site-packages/deepspeed/ops/csrc/includes -isystem /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/include -isystem /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/v2129375/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/v2129375/miniconda3/lib/python3.12/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[2/3] /home/v2129375/miniconda3/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/home/v2129375/miniconda3/lib/python3.12/site-packages/deepspeed/ops/csrc/includes -isystem /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/include -isystem /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/v2129375/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/v2129375/miniconda3/lib/python3.12/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[3/3] /home/v2129375/miniconda3/bin/x86_64-conda-linux-gnu-c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 15.975576400756836 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 6.151668310165405 seconds\n",
      "ntumir-System-Product-Name:1376531:1376531 [0] NCCL INFO Comm config Blocking set to 1\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Using network Socket\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO ncclCommInitRankConfig comm 0xf1556c70 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0xf0ccd49101422638 - Init START\n",
      "ntumir-System-Product-Name:1376532:1376532 [1] NCCL INFO Comm config Blocking set to 1\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO Using network Socket\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO ncclCommInitRankConfig comm 0x28854e50 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2000 commId 0xf0ccd49101422638 - Init START\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Bootstrap timings total 0.053332 (create 0.000018, send 0.000063, recv 0.053126, ring 0.000015, delay 0.000000)\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO Bootstrap timings total 0.000323 (create 0.000021, send 0.000057, recv 0.000143, ring 0.000012, delay 0.000000)\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO comm 0x28854e50 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO comm 0xf1556c70 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Channel 00/02 : 0 1\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Channel 01/02 : 0 1\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0\n",
      "ntumir-System-Product-Name:1376531:1377603 [0] NCCL INFO [Proxy Service] Device 0 CPU core 26\n",
      "ntumir-System-Product-Name:1376532:1377602 [1] NCCL INFO [Proxy Service] Device 1 CPU core 26\n",
      "ntumir-System-Product-Name:1376532:1377604 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 23\n",
      "ntumir-System-Product-Name:1376531:1377605 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 16\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO ncclCommInitRankConfig comm 0xf1556c70 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0xf0ccd49101422638 - Init COMPLETE\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO ncclCommInitRankConfig comm 0x28854e50 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2000 commId 0xf0ccd49101422638 - Init COMPLETE\n",
      "ntumir-System-Product-Name:1376531:1377598 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.06 (kernels 0.00, alloc 0.00, bootstrap 0.05, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "ntumir-System-Product-Name:1376532:1377601 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.00 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "ntumir-System-Product-Name:1376531:1377607 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376532:1377606 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376531:1377607 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376532:1377606 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\n",
      "ntumir-System-Product-Name:1376532:1377606 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "ntumir-System-Product-Name:1376531:1377607 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  0%|                                                   | 0/378 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 3.2664, 'grad_norm': 24.014978408813477, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}\n",
      "{'loss': 0.6419, 'grad_norm': 0.17322637140750885, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0899, 'grad_norm': 4.592565059661865, 'learning_rate': 2.4e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0983, 'grad_norm': 1.6162595748901367, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2011, 'grad_norm': 0.1305372416973114, 'learning_rate': 4e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2406, 'grad_norm': 9.093375205993652, 'learning_rate': 3.878048780487805e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3644, 'grad_norm': 0.2579725384712219, 'learning_rate': 3.75609756097561e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7753, 'grad_norm': 10.886005401611328, 'learning_rate': 3.634146341463415e-05, 'epoch': 0.21}\n",
      "{'loss': 0.6522, 'grad_norm': 2.0989441871643066, 'learning_rate': 3.51219512195122e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2034, 'grad_norm': 0.06170574203133583, 'learning_rate': 3.390243902439025e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2705, 'grad_norm': 0.01399520318955183, 'learning_rate': 3.268292682926829e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4844, 'grad_norm': 8.579767227172852, 'learning_rate': 3.146341463414635e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1236, 'grad_norm': 8.120859146118164, 'learning_rate': 3.0243902439024392e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1159, 'grad_norm': 2.7399704456329346, 'learning_rate': 2.902439024390244e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4782, 'grad_norm': 10.398323059082031, 'learning_rate': 2.7804878048780487e-05, 'epoch': 0.4}\n",
      "{'loss': 0.211, 'grad_norm': 0.7168093323707581, 'learning_rate': 2.658536585365854e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3928, 'grad_norm': 4.087973594665527, 'learning_rate': 2.536585365853659e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2618, 'grad_norm': 0.23531635105609894, 'learning_rate': 2.4146341463414634e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3083, 'grad_norm': 3.816931962966919, 'learning_rate': 2.2926829268292683e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2507, 'grad_norm': 4.312002182006836, 'learning_rate': 2.1707317073170736e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1204, 'grad_norm': 7.798188209533691, 'learning_rate': 2.048780487804878e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3097, 'grad_norm': 7.470046043395996, 'learning_rate': 1.926829268292683e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1342, 'grad_norm': 16.488435745239258, 'learning_rate': 1.804878048780488e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2711, 'grad_norm': 23.942737579345703, 'learning_rate': 1.682926829268293e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0848, 'grad_norm': 0.006024249829351902, 'learning_rate': 1.5609756097560978e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1371, 'grad_norm': 0.02697915770113468, 'learning_rate': 1.4390243902439025e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1853, 'grad_norm': 0.6480385065078735, 'learning_rate': 1.3170731707317076e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1714, 'grad_norm': 0.008692110888659954, 'learning_rate': 1.1951219512195123e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3176, 'grad_norm': 23.37497329711914, 'learning_rate': 1.0731707317073172e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2693, 'grad_norm': 0.01807332970201969, 'learning_rate': 9.51219512195122e-06, 'epoch': 0.79}\n",
      "{'loss': 0.085, 'grad_norm': 0.03307739272713661, 'learning_rate': 8.292682926829268e-06, 'epoch': 0.82}\n",
      "{'loss': 0.1521, 'grad_norm': 4.3924736976623535, 'learning_rate': 7.0731707317073175e-06, 'epoch': 0.85}\n",
      "{'loss': 0.4831, 'grad_norm': 0.05268053710460663, 'learning_rate': 5.853658536585366e-06, 'epoch': 0.87}\n",
      "{'loss': 0.0166, 'grad_norm': 0.6849682927131653, 'learning_rate': 4.634146341463416e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1886, 'grad_norm': 0.03917662799358368, 'learning_rate': 3.414634146341464e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0732, 'grad_norm': 1.4556087255477905, 'learning_rate': 2.1951219512195125e-06, 'epoch': 0.95}\n",
      "{'loss': 0.2743, 'grad_norm': 0.09163776785135269, 'learning_rate': 9.75609756097561e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 330.4629, 'train_samples_per_second': 2.285, 'train_steps_per_second': 1.144, 'train_loss': 0.33760475741807744, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 378/378 [05:30<00:00,  1.14it/s]\n",
      "Training completed successfully!\n",
      "[rank0]:[W528 14:57:56.393743363 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ntumir-System-Product-Name:1376531:1377074 [0] NCCL INFO misc/socket.cc:881 -> 3\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ntumir-System-Product-Name:1376532:1377073 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "ntumir-System-Product-Name:1376531:1377074 [0] NCCL INFO misc/socket.cc:881 -> 3\n",
      "ntumir-System-Product-Name:1376531:1385396 [0] NCCL INFO comm 0x12f06e10 rank 0 nranks 2 cudaDev 0 busId 1000 - Abort COMPLETE\n",
      "ntumir-System-Product-Name:1376532:1377602 [1] NCCL INFO [Service thread] Connection closed by localRank 0\n",
      "ntumir-System-Product-Name:1376532:1385398 [1] NCCL INFO comm 0x23aa4df0 rank 1 nranks 2 cudaDev 1 busId 2000 - Abort COMPLETE\n"
     ]
    }
   ],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1 \\\n",
    "    NCCL_DEBUG=INFO \\\n",
    "    FLASH_ATTENTION_2_DISABLE=0 \\\n",
    "    NCCL_P2P_DISABLE=0 \\\n",
    "    NCCL_SHM_DISABLE=0 \\\n",
    "    NCCL_P2P_LEVEL=NVL \\\n",
    "    accelerate launch asr/finetune/finetune_speech_asr_keywords3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
