{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/asr_biasing\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  6.16it/s]\n",
      "数据集中发现的领域: ['video']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "Train dataset size: 1510\n",
      "Eval dataset size: 1\n",
      "Keywords directory: data/catslu\n",
      "training on 1 GPUs\n",
      "running eval:   0%|                                       | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "running eval: 100%|███████████████████████████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Overall CER: 0.5000\n",
      "Overall Keyword Error Rate: 0.0000\n",
      "\n",
      "按领域统计:\n",
      "VIDEO - Count: 1, CER: 0.5000\n",
      "  Keyword Error Rate: 0.0000 (0/1)\n",
      "\n",
      "错误识别的样本:\n",
      "CER before finetuning: 0.5\n",
      "Keyword Error Rate before finetuning: 0.0\n",
      "[2025-05-27 03:05:08,392] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "  0%|                                                   | 0/377 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!python /home/v2129375/asr_biasing/asr/finetune/finetune_speech_asr_keywords3.py \\\n",
    "    --use_flash_attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA版本: 12.8\n",
      "CUDA版本: 12.8\n",
      "可用GPU数量: 2\n",
      "GPU 0: NVIDIA GeForce RTX 5080, 内存: 15.5GB\n",
      "GPU 1: NVIDIA GeForce RTX 5080, 内存: 15.5GB\n",
      "可用GPU数量: 2\n",
      "GPU 0: NVIDIA GeForce RTX 5080, 内存: 15.5GB\n",
      "GPU 1: NVIDIA GeForce RTX 5080, 内存: 15.5GB\n",
      "[rank1]:[W526 22:56:52.394622522 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "主进程加载模型...\n",
      "检测到 2 个GPU\n",
      "强制关闭FlashAttention，使用fp16量化训练\n",
      "分布式训练模式: True\n",
      "使用分布式运行，fp16量化\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.41s/it]\n",
      "模型加载完成，准备分布式训练...\n",
      "[rank0]:[W526 22:57:07.210732985 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "检测到 2 个GPU\n",
      "强制关闭FlashAttention，使用fp16量化训练\n",
      "分布式训练模式: True\n",
      "使用分布式运行，fp16量化\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.42s/it]\n",
      "模型加载完成，准备分布式训练...\n",
      "检测到 2 个GPU，当前训练进程数: 2\n",
      "检测到 2 个GPU，当前训练进程数: 2\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "Train dataset size: 1511\n",
      "不执行验证\n",
      "Keywords directory: data/catslu\n",
      "有效训练GPU数量: 2\n",
      "警告: 批处理大小 1 不能被 (GPU数量 2 * 每GPU批处理大小 1) 整除\n",
      "自动调整批处理大小为: 2\n",
      "梯度累积步数: 1\n",
      "使用fp16精度训练\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "有效训练GPU数量: 2\n",
      "警告: 批处理大小 1 不能被 (GPU数量 2 * 每GPU批处理大小 1) 整除\n",
      "自动调整批处理大小为: 2\n",
      "梯度累积步数: 1\n",
      "使用fp16精度训练\n",
      "[2025-05-26 22:57:24,042] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 22:57:24,147] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[rank1]:[E526 22:57:24.849084389 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fe8943785e8 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7fe89430d4a2 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7fe894807422 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7fe823ae5456 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7fe823af56f0 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7fe823af7282 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe823af8e8d in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #7: <unknown function> + 0xdbbf4 (0x7fe8980dbbf4 in /home/v2129375/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #8: <unknown function> + 0x9caa4 (0x7fe89b69caa4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #9: <unknown function> + 0x129c3c (0x7fe89b729c3c in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank0]:[E526 22:57:24.874055932 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7c2d611785e8 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7c2d6110d4a2 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7c2d615e2422 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7c2cf08e5456 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7c2cf08f56f0 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7c2cf08f7282 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c2cf08f8e8d in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #7: <unknown function> + 0xdbbf4 (0x7c2d64edbbf4 in /home/v2129375/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #8: <unknown function> + 0x9caa4 (0x7c2d6849caa4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #9: <unknown function> + 0x129c3c (0x7c2d68529c3c in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "W0526 22:57:26.228000 30260 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 30292 closing signal SIGTERM\n",
      "E0526 22:57:27.195000 30260 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 1 (pid: 30293) of binary: /home/v2129375/miniconda3/bin/python3.12\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/v2129375/miniconda3/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/accelerate/commands/launch.py\", line 1163, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/accelerate/commands/launch.py\", line 792, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "======================================================\n",
      "asr/finetune/finetune_speech_asr_keywords2.py FAILED\n",
      "------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-26_22:57:26\n",
      "  host      : ntumir-System-Product-Name\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : -6 (pid: 30293)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 6 (SIGABRT) received by PID 30293\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1 accelerate launch \\\n",
    "    --mixed_precision=fp16 \\\n",
    "    --multi_gpu \\\n",
    "    --num_processes=2 \\\n",
    "    --num_machines=1 \\\n",
    "    --dynamo_backend=no \\\n",
    "    --main_process_port=29500 \\\n",
    "    asr/finetune/finetune_speech_asr_keywords2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank1]:[W527 03:43:20.046561359 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  5.81it/s]\n",
      "[rank0]:[W527 03:43:30.155846944 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "Train dataset size: 1511\n",
      "Evaluation is disabled (_EVAL_SIZE is None)\n",
      "Keywords directory: data/catslu\n",
      "training on 2 GPUs\n",
      "/home/v2129375/miniconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/home/v2129375/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  6.16it/s]\n",
      "数据集中发现的领域: ['video' 'music' 'city']\n",
      "Loaded 555 keywords for video domain\n",
      "Loaded 390 keywords for music domain\n",
      "Loaded 930 keywords for city domain\n",
      "training on 2 GPUs\n",
      "[rank1]:[W527 03:43:42.948709902 CUDAGuardImpl.h:119] Warning: CUDA warning: an illegal memory access was encountered (function destroyEvent)\n",
      "[rank0]:[W527 03:43:42.949304856 CUDAGuardImpl.h:119] Warning: CUDA warning: an illegal memory access was encountered (function destroyEvent)\n",
      "[rank1]:[E527 03:43:42.949397060 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7d9c27b785e8 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7d9c27b0d4a2 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7d9c34371422 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7d9bc3ae5456 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7d9bc3af56f0 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7d9bc3af7282 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d9bc3af8e8d in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #7: <unknown function> + 0xdbbf4 (0x7d9c37edbbf4 in /home/v2129375/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #8: <unknown function> + 0x9caa4 (0x7d9c3b49caa4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #9: <unknown function> + 0x129c3c (0x7d9c3b529c3c in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank0]:[E527 03:43:42.949998386 ProcessGroupNCCL.cpp:1896] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7cc745b785e8 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7cc745b0d4a2 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7cc745fe2422 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7cc6d52e5456 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7cc6d52f56f0 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7cc6d52f7282 in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cc6d52f8e8d in /home/v2129375/miniconda3/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #7: <unknown function> + 0xdbbf4 (0x7cc7498dbbf4 in /home/v2129375/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #8: <unknown function> + 0x9caa4 (0x7cc74ce9caa4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #9: <unknown function> + 0x129c3c (0x7cc74cf29c3c in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "W0527 03:43:44.248000 82059 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 82114 closing signal SIGTERM\n",
      "E0527 03:43:45.316000 82059 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 0 (pid: 82113) of binary: /home/v2129375/miniconda3/bin/python3.12\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/v2129375/miniconda3/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/accelerate/commands/launch.py\", line 1163, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/accelerate/commands/launch.py\", line 792, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v2129375/miniconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "======================================================\n",
      "asr/finetune/finetune_speech_asr_keywords3.py FAILED\n",
      "------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-27_03:43:44\n",
      "  host      : ntumir-System-Product-Name\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : -6 (pid: 82113)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 6 (SIGABRT) received by PID 82113\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch asr/finetune/finetune_speech_asr_keywords3.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
