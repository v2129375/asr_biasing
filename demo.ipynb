{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/asr_biasing\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/v2129375/anaconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:01<00:00,  2.74it/s]\n",
      "Train dataset size: 1311\n",
      "Eval dataset size: 200\n",
      "training on 1 GPUs\n",
      "running eval:   0%|                                     | 0/200 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/v2129375/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "running eval: 100%|███████████████████████████| 200/200 [00:50<00:00,  3.96it/s]\n",
      "CER: 0.1844\n",
      "Keyword Error Rate: 0.1200\n",
      "\n",
      "错误识别的样本:\n",
      "原始文本: 我想看大头儿子小头爸爸\n",
      "识别结果: 我想看大头儿子，小头爸爸。\n",
      "关键词: 大头儿子小头爸爸\n",
      "---\n",
      "原始文本: 我要听温岚的囚鸟\n",
      "识别结果: 我要听温暖的秋天哦\n",
      "关键词: 温岚\n",
      "---\n",
      "原始文本: 我想看汪汪队动画片\n",
      "识别结果: 我想看弯弯队动画片\n",
      "关键词: 汪汪队\n",
      "---\n",
      "原始文本: 泰州的天气\n",
      "识别结果: 台州的天气。\n",
      "关键词: 泰州\n",
      "---\n",
      "原始文本: 放一首刘欢的青花瓷\n",
      "识别结果: 唱一首刘芳的青花瓷。\n",
      "关键词: 刘欢\n",
      "---\n",
      "原始文本: 赌霸\n",
      "识别结果: 赌吧\n",
      "关键词: 赌霸\n",
      "---\n",
      "原始文本: 播放艾利之书\n",
      "识别结果: 播放爱丽之书。\n",
      "关键词: 艾利之书\n",
      "---\n",
      "原始文本: 芭比\n",
      "识别结果: Papi.\n",
      "关键词: 芭比\n",
      "---\n",
      "原始文本: 播放刘洪刚的歌\n",
      "识别结果: 播放刘恒刚的歌。\n",
      "关键词: 刘洪刚\n",
      "---\n",
      "原始文本: 播放周渝民的记得我爱你\n",
      "识别结果: 播放周昱铭的记得我爱你。\n",
      "关键词: 周渝民\n",
      "---\n",
      "CER before finetuning: 0.1844081751581753\n",
      "Keyword Error Rate before finetuning: 0.12\n",
      "  0%|                                                  | 0/1311 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "{'loss': 3.1896, 'grad_norm': 34.5, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}\n",
      "{'loss': 2.0534, 'grad_norm': 22.25, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4164, 'grad_norm': 25.875, 'learning_rate': 2.4e-05, 'epoch': 0.02}  \n",
      "{'loss': 0.6488, 'grad_norm': 0.1298828125, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0678, 'grad_norm': 15.1875, 'learning_rate': 4e-05, 'epoch': 0.04}   \n",
      "{'loss': 0.4393, 'grad_norm': 0.0169677734375, 'learning_rate': 3.968279143536876e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0255, 'grad_norm': 19.75, 'learning_rate': 3.936558287073751e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9936, 'grad_norm': 17.125, 'learning_rate': 3.904837430610627e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0295, 'grad_norm': 6.53125, 'learning_rate': 3.873116574147502e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6457, 'grad_norm': 7.5625, 'learning_rate': 3.841395717684378e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1292, 'grad_norm': 2.3125, 'learning_rate': 3.809674861221253e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0995, 'grad_norm': 1.03125, 'learning_rate': 3.7779540047581286e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2043, 'grad_norm': 18.0, 'learning_rate': 3.7462331482950045e-05, 'epoch': 0.1}\n",
      "{'loss': 0.302, 'grad_norm': 0.26171875, 'learning_rate': 3.71451229183188e-05, 'epoch': 0.11}\n",
      "{'loss': 0.6509, 'grad_norm': 0.0703125, 'learning_rate': 3.682791435368755e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1089, 'grad_norm': 0.0118408203125, 'learning_rate': 3.651070578905631e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0443, 'grad_norm': 0.00982666015625, 'learning_rate': 3.619349722442506e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0615, 'grad_norm': 0.06689453125, 'learning_rate': 3.587628865979382e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0935, 'grad_norm': 1.25, 'learning_rate': 3.555908009516257e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2132, 'grad_norm': 5.125, 'learning_rate': 3.524187153053133e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0137, 'grad_norm': 4.8125, 'learning_rate': 3.492466296590008e-05, 'epoch': 0.16}\n",
      "{'loss': 0.523, 'grad_norm': 71.5, 'learning_rate': 3.460745440126884e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0734, 'grad_norm': 2.671875, 'learning_rate': 3.429024583663759e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4282, 'grad_norm': 0.00128936767578125, 'learning_rate': 3.3973037272006345e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2142, 'grad_norm': 5.46875, 'learning_rate': 3.3655828707375104e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5298, 'grad_norm': 0.014404296875, 'learning_rate': 3.333862014274386e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4963, 'grad_norm': 8.5625, 'learning_rate': 3.302141157811261e-05, 'epoch': 0.21}\n",
      "{'loss': 0.493, 'grad_norm': 0.056396484375, 'learning_rate': 3.270420301348137e-05, 'epoch': 0.21}\n",
      "{'loss': 0.1733, 'grad_norm': 16.625, 'learning_rate': 3.238699444885012e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0698, 'grad_norm': 0.0181884765625, 'learning_rate': 3.206978588421888e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0446, 'grad_norm': 2.078125, 'learning_rate': 3.175257731958763e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5196, 'grad_norm': 0.06396484375, 'learning_rate': 3.1435368754956386e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0866, 'grad_norm': 0.0164794921875, 'learning_rate': 3.111816019032514e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0588, 'grad_norm': 0.0067138671875, 'learning_rate': 3.08009516256939e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2137, 'grad_norm': 0.002716064453125, 'learning_rate': 3.048374306106265e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1042, 'grad_norm': 0.056396484375, 'learning_rate': 3.0166534496431407e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0943, 'grad_norm': 0.71484375, 'learning_rate': 2.9849325931800163e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0884, 'grad_norm': 0.34375, 'learning_rate': 2.953211736716892e-05, 'epoch': 0.29}\n",
      "{'loss': 0.313, 'grad_norm': 0.004486083984375, 'learning_rate': 2.921490880253767e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1521, 'grad_norm': 0.02734375, 'learning_rate': 2.8897700237906425e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0969, 'grad_norm': 0.07275390625, 'learning_rate': 2.858049167327518e-05, 'epoch': 0.31}\n",
      "{'loss': 0.222, 'grad_norm': 1.9921875, 'learning_rate': 2.8263283108643937e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0298, 'grad_norm': 0.06396484375, 'learning_rate': 2.794607454401269e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6996, 'grad_norm': 24.125, 'learning_rate': 2.7628865979381445e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1935, 'grad_norm': 5.625, 'learning_rate': 2.73116574147502e-05, 'epoch': 0.34}\n",
      "{'loss': 0.059, 'grad_norm': 0.92578125, 'learning_rate': 2.6994448850118958e-05, 'epoch': 0.35}\n",
      "{'loss': 0.037, 'grad_norm': 0.061767578125, 'learning_rate': 2.667724028548771e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0128, 'grad_norm': 12.125, 'learning_rate': 2.6360031720856466e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0323, 'grad_norm': 8.125, 'learning_rate': 2.6042823156225222e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3468, 'grad_norm': 10.75, 'learning_rate': 2.572561459159398e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0545, 'grad_norm': 0.00101470947265625, 'learning_rate': 2.5408406026962728e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0136, 'grad_norm': 9.0, 'learning_rate': 2.5091197462331484e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0726, 'grad_norm': 0.002410888671875, 'learning_rate': 2.477398889770024e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0023, 'grad_norm': 0.8828125, 'learning_rate': 2.4456780333068996e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0551, 'grad_norm': 0.031005859375, 'learning_rate': 2.413957176843775e-05, 'epoch': 0.42}\n",
      "{'loss': 0.7507, 'grad_norm': 4.3125, 'learning_rate': 2.3822363203806505e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0595, 'grad_norm': 0.046875, 'learning_rate': 2.350515463917526e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3304, 'grad_norm': 0.00052642822265625, 'learning_rate': 2.3187946074544017e-05, 'epoch': 0.44}\n",
      "{'loss': 0.5433, 'grad_norm': 0.0004444122314453125, 'learning_rate': 2.287073750991277e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1655, 'grad_norm': 19.25, 'learning_rate': 2.2553528945281525e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1195, 'grad_norm': 37.75, 'learning_rate': 2.223632038065028e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1767, 'grad_norm': 0.0595703125, 'learning_rate': 2.1919111816019037e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1987, 'grad_norm': 0.0673828125, 'learning_rate': 2.1601903251387787e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4016, 'grad_norm': 0.0140380859375, 'learning_rate': 2.1284694686756543e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0753, 'grad_norm': 25.75, 'learning_rate': 2.09674861221253e-05, 'epoch': 0.5}\n",
      "{'loss': 0.1751, 'grad_norm': 20.375, 'learning_rate': 2.0650277557494055e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0078, 'grad_norm': 0.15625, 'learning_rate': 2.0333068992862808e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1479, 'grad_norm': 16.75, 'learning_rate': 2.0015860428231564e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0171, 'grad_norm': 0.00714111328125, 'learning_rate': 1.969865186360032e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0053, 'grad_norm': 0.048583984375, 'learning_rate': 1.9381443298969072e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7042, 'grad_norm': 0.0096435546875, 'learning_rate': 1.906423473433783e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3283, 'grad_norm': 0.0146484375, 'learning_rate': 1.8747026169706585e-05, 'epoch': 0.55}\n",
      "{'loss': 0.1761, 'grad_norm': 0.08251953125, 'learning_rate': 1.8429817605075337e-05, 'epoch': 0.56}\n",
      "{'loss': 0.024, 'grad_norm': 0.00347900390625, 'learning_rate': 1.8112609040444093e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0097, 'grad_norm': 0.009521484375, 'learning_rate': 1.7795400475812846e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0413, 'grad_norm': 0.1650390625, 'learning_rate': 1.7478191911181602e-05, 'epoch': 0.58}\n",
      "{'loss': 0.271, 'grad_norm': 0.224609375, 'learning_rate': 1.7160983346550358e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7591, 'grad_norm': 0.00225830078125, 'learning_rate': 1.6843774781919114e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2426, 'grad_norm': 10.875, 'learning_rate': 1.6526566217287867e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1126, 'grad_norm': 0.06689453125, 'learning_rate': 1.6209357652656623e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1292, 'grad_norm': 1.25, 'learning_rate': 1.5892149088025375e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1057, 'grad_norm': 0.09521484375, 'learning_rate': 1.557494052339413e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3176, 'grad_norm': 10.25, 'learning_rate': 1.5257731958762888e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0403, 'grad_norm': 0.0037078857421875, 'learning_rate': 1.4940523394131644e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3135, 'grad_norm': 2.3125, 'learning_rate': 1.4623314829500396e-05, 'epoch': 0.65}\n",
      "{'loss': 0.102, 'grad_norm': 0.000335693359375, 'learning_rate': 1.4306106264869152e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0633, 'grad_norm': 0.00335693359375, 'learning_rate': 1.3988897700237907e-05, 'epoch': 0.66}\n",
      "{'loss': 0.4191, 'grad_norm': 0.0033416748046875, 'learning_rate': 1.3671689135606663e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2196, 'grad_norm': 5.96875, 'learning_rate': 1.3354480570975417e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3434, 'grad_norm': 24.0, 'learning_rate': 1.3037272006344173e-05, 'epoch': 0.69}\n",
      "{'loss': 0.162, 'grad_norm': 15.5625, 'learning_rate': 1.2720063441712926e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1025, 'grad_norm': 0.0260009765625, 'learning_rate': 1.2402854877081682e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2467, 'grad_norm': 0.0064697265625, 'learning_rate': 1.2085646312450436e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0465, 'grad_norm': 0.006500244140625, 'learning_rate': 1.1768437747819192e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0462, 'grad_norm': 0.034423828125, 'learning_rate': 1.1451229183187947e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0006, 'grad_norm': 0.002532958984375, 'learning_rate': 1.1134020618556703e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2848, 'grad_norm': 0.248046875, 'learning_rate': 1.0816812053925455e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0995, 'grad_norm': 0.005584716796875, 'learning_rate': 1.0499603489294211e-05, 'epoch': 0.75}\n",
      "{'loss': 0.1884, 'grad_norm': 0.90625, 'learning_rate': 1.0182394924662966e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1807, 'grad_norm': 0.01953125, 'learning_rate': 9.865186360031722e-06, 'epoch': 0.76}\n",
      "{'loss': 0.5074, 'grad_norm': 0.0003032684326171875, 'learning_rate': 9.547977795400476e-06, 'epoch': 0.77}\n",
      "{'loss': 0.361, 'grad_norm': 0.01123046875, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.78}\n",
      "{'loss': 0.0051, 'grad_norm': 0.002593994140625, 'learning_rate': 8.913560666137987e-06, 'epoch': 0.79}\n",
      "{'loss': 0.4464, 'grad_norm': 0.00089263916015625, 'learning_rate': 8.596352101506743e-06, 'epoch': 0.79}\n",
      "{'loss': 0.0612, 'grad_norm': 0.0084228515625, 'learning_rate': 8.279143536875495e-06, 'epoch': 0.8}\n",
      "{'loss': 0.0968, 'grad_norm': 0.0125732421875, 'learning_rate': 7.961934972244251e-06, 'epoch': 0.81}\n",
      "{'loss': 0.1492, 'grad_norm': 8.125, 'learning_rate': 7.644726407613006e-06, 'epoch': 0.82}\n",
      "{'loss': 0.0489, 'grad_norm': 8.6875, 'learning_rate': 7.327517842981761e-06, 'epoch': 0.82}\n",
      "{'loss': 0.3581, 'grad_norm': 0.2890625, 'learning_rate': 7.010309278350515e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1508, 'grad_norm': 0.56640625, 'learning_rate': 6.693100713719271e-06, 'epoch': 0.84}\n",
      "{'loss': 0.007, 'grad_norm': 0.004730224609375, 'learning_rate': 6.375892149088026e-06, 'epoch': 0.85}\n",
      "{'loss': 0.1176, 'grad_norm': 0.00064849853515625, 'learning_rate': 6.05868358445678e-06, 'epoch': 0.85}\n",
      "{'loss': 0.8283, 'grad_norm': 0.0027313232421875, 'learning_rate': 5.741475019825535e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1099, 'grad_norm': 12.9375, 'learning_rate': 5.424266455194291e-06, 'epoch': 0.87}\n",
      "{'loss': 0.2055, 'grad_norm': 10.9375, 'learning_rate': 5.107057890563045e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0724, 'grad_norm': 0.00482177734375, 'learning_rate': 4.789849325931801e-06, 'epoch': 0.88}\n",
      "{'loss': 0.1838, 'grad_norm': 0.00054931640625, 'learning_rate': 4.472640761300555e-06, 'epoch': 0.89}\n",
      "{'loss': 0.3847, 'grad_norm': 0.004974365234375, 'learning_rate': 4.1554321966693106e-06, 'epoch': 0.9}\n",
      "{'loss': 0.2194, 'grad_norm': 0.004150390625, 'learning_rate': 3.838223632038066e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0081, 'grad_norm': 0.0072021484375, 'learning_rate': 3.5210150674068206e-06, 'epoch': 0.92}\n",
      "{'loss': 0.0124, 'grad_norm': 4.53125, 'learning_rate': 3.2038065027755754e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2129, 'grad_norm': 37.25, 'learning_rate': 2.8865979381443297e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0097, 'grad_norm': 0.000904083251953125, 'learning_rate': 2.569389373513085e-06, 'epoch': 0.94}\n",
      "{'loss': 0.2341, 'grad_norm': 8.1875, 'learning_rate': 2.25218080888184e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0012, 'grad_norm': 0.02490234375, 'learning_rate': 1.934972244250595e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0171, 'grad_norm': 4.71875, 'learning_rate': 1.61776367961935e-06, 'epoch': 0.96}\n",
      "{'loss': 0.0055, 'grad_norm': 0.0233154296875, 'learning_rate': 1.300555114988105e-06, 'epoch': 0.97}\n",
      "{'loss': 0.1088, 'grad_norm': 0.005218505859375, 'learning_rate': 9.833465503568597e-07, 'epoch': 0.98}\n",
      "{'loss': 0.2318, 'grad_norm': 2.953125, 'learning_rate': 6.661379857256146e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5032, 'grad_norm': 0.0115966796875, 'learning_rate': 3.489294210943696e-07, 'epoch': 0.99}\n",
      "{'loss': 0.1261, 'grad_norm': 0.1669921875, 'learning_rate': 3.1720856463124506e-08, 'epoch': 1.0}\n",
      "{'train_runtime': 209.5154, 'train_samples_per_second': 6.257, 'train_steps_per_second': 6.257, 'train_loss': 0.24465027691935232, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 1311/1311 [03:29<00:00,  6.26it/s]\n",
      "/home/v2129375/anaconda3/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "/root/.cache/huggingface/modules/transformers_modules/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  5.81it/s]\n",
      "running eval:   0%|                                     | 0/200 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/v2129375/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "running eval: 100%|███████████████████████████| 200/200 [00:45<00:00,  4.43it/s]\n",
      "CER: 0.0345\n",
      "Keyword Error Rate: 0.0750\n",
      "\n",
      "错误识别的样本:\n",
      "原始文本: 我要看猩球崛起\n",
      "识别结果: 我要看星球崛起\n",
      "关键词: 猩球崛起\n",
      "---\n",
      "原始文本: 泰州的天气\n",
      "识别结果: 台州的天气\n",
      "关键词: 泰州\n",
      "---\n",
      "原始文本: 播放电视剧大染坊\n",
      "识别结果: 播放电视剧大染房\n",
      "关键词: 大染坊\n",
      "---\n",
      "原始文本: 赌霸\n",
      "识别结果: 赌吧\n",
      "关键词: 赌霸\n",
      "---\n",
      "原始文本: 芭比\n",
      "识别结果: papi\n",
      "关键词: 芭比\n",
      "---\n",
      "原始文本: 播放刘洪刚的歌\n",
      "识别结果: 播放刘和刚的歌\n",
      "关键词: 刘洪刚\n",
      "---\n",
      "原始文本: 播放周渝民的记得我爱你\n",
      "识别结果: 播放周玉明的记得我爱你\n",
      "关键词: 周渝民\n",
      "---\n",
      "原始文本: 小乐你能唱一首杨沛宜的左手右手\n",
      "识别结果: 小乐你能唱一首杨培仪的左手右手\n",
      "关键词: 杨沛宜\n",
      "---\n",
      "原始文本: 陈娇的猴哥回来了\n",
      "识别结果: 成交的猴歌回来了\n",
      "关键词: 陈娇\n",
      "---\n",
      "原始文本: 来一首梁博的日落大道\n",
      "识别结果: 来一首凉薄的日落大道\n",
      "关键词: 梁博\n",
      "---\n",
      "CER after finetuning: 0.03445880508380508\n",
      "Keyword Error Rate after finetuning: 0.075\n"
     ]
    }
   ],
   "source": [
    "!/home/v2129375/anaconda3/bin/python /home/v2129375/asr_biasing/asr/sample_finetune_speech_asr.py \\\n",
    "    --use_flash_attention "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
